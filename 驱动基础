一、 Linux 系统与驱动基础 
1. 问题：为了诊断一个疑似Linux驱动问题，你需要查看内核日志。你会使用什么命令？
如何实时监控日志，并过滤出只与你测试的驱动（例如一个叫`my_driver.ko``）相关的信息？

使用 dmesg -w 或 tail -f /var/log/kerner.log
dmesg -w | grep -i my_driver
插入驱动模块时调整 printk 的日志级别，或者使用 udevadm 等工具来监控设备事件

调整printk日志级别（动态修改）：
# 查看当前控制台日志级别
cat /proc/sys/kernel/printk
# 临时设置控制台日志级别为7（调试级别）
echo 7 > /proc/sys/kernel/printk
# 插入模块时指定调试级别（示例）
sudo insmod mymodule.ko debug=1

内核模块中设置printk级别（源码级）：
// 在驱动代码中添加不同级别的日志
printk(KERN_DEBUG "Debug message\n");    // 7
printk(KERN_INFO "Info message\n");      // 6
printk(KERN_NOTICE "Notice message\n");  // 5
printk(KERN_WARNING "Warning message\n");// 4
printk(KERN_ERR "Error message\n");      // 3

使用udevadm监控设备事件：
# 实时监控所有udev事件
sudo udevadm monitor --kernel --property --udev
# 监控特定子系统的事件（如USB）
sudo udevadm monitor --kernel --property --subsystem-match=usb
# 查看设备属性（以sda为例）
udevadm info --query=all --name=/dev/sda
# 触发设备事件（用于测试规则）
sudo udevadm trigger --verbose --action=add --subsystem-match=usb

实例场景：
调试USB驱动加载：
# 设置最高日志级别
echo 8 > /proc/sys/kernel/printk
# 插入驱动并监控
sudo insmod usb_driver.ko
udevadm monitor --property --kernel --subsystem-match=usb

监控鼠标插拔事件：
udevadm monitor --property --kernel --subsystem-match=input | grep --line-buffered "MOUSE"
-----------------------------------------------------------------------
2. 问题：一个驱动模块（.ko文件）无法正常加载（insmod），你通常会从哪几个方面去排查问题？

查看内核日志：
首先运行 dmesg | tail，加载失败的原因通常会在日志末尾有明确提示，
比如 ‘Unknown symbol’（未知符号）或 ‘Invalid parameter’（参数无效）。

检查模块依赖：
使用 modinfo my_driver.ko 查看模块信息，特别是 depends 字段。
如果它有依赖的其他模块，需要先用 insmod 或 modprobe 加载这些依赖模块。
modprobe 会自动处理依赖关系，通常是更好的选择。

检查符号丢失：
如果日志提示 ‘Unknown symbol’，意味着该模块依赖的某个函数或变量在内核或其他模块中找不到。
这可能是依赖模块没加载，或者是内核配置（.config）不同导致的版本不匹配。

验证模块版本：
使用 uname -r 确认当前运行的内核版本，并检查模块是否是为这个版本的内核编译的。版本不匹配是常见问题。

检查参数：
如果驱动接受参数，检查传递给 insmod 的参数是否正确、有效。
modinfo your_module.ko
insmod your_module.ko param1=value1 param2=value2
dmesg | tail -20   # 查看最后20行日志

# 启用动态调试后加载模块
echo "module your_module +p" > /sys/kernel/debug/dynamic_debug/control
sudo insmod your_module.ko param1=test
dmesg | grep your_module

# 测试边界值（示例）
sudo insmod your_module.ko int_param=2147483647  # INT_MAX测试
sudo insmod your_module.ko int_param=2147483648  # 应失败

# 加载后检查参数实际值
cat /sys/module/your_module/parameters/*

# 验证参数变化后的重新加载
sudo rmmod your_module
sudo insmod your_module.ko changed_param=new_value
---------------------------------------------------------------------------
二、 测试设计与流程 (考察测试思维和流程熟悉度)
3. 问题：请描述一下，当你拿到一个全新的Linux驱动需求文档时，你的测试设计思路是怎样的？你会从哪些方面来设计测试用例？
功能测试：这是核心。
我会逐条验证需求文档中定义的每个功能点。
包括正常的输入输出、异常和错误处理（如传递非法参数、设备断开等）、边界值测试。

性能测试：
评估驱动的性能指标，如吞吐量、延迟、CPU占用率、内存占用等。
我会使用 perf, ftrace, iostat 等工具进行 profiling，找出性能瓶颈。

稳定性/压力测试：
长时间、高负载地运行驱动，看是否会出现内存泄漏、死锁、系统崩溃（panic/oops）等问题。
例如，连续进行百万次的设备打开-关闭操作。

兼容性测试：
在不同内核版本、不同硬件平台（如x86 vs ARM）、不同编译器版本上进行测试。

安全测试：
检查是否有权限绕过、缓冲区溢出等安全问题。

升级与降级测试：
验证驱动模块或固件升级、降级后功能是否正常。

异常测试：
模拟异常场景，如热插拔设备、突然断电后上电、资源（内存）耗尽等情况下的驱动行为。”
---------------------------------------------------------------------------
4. 问题：你发现了一个Bug并提交了，但开发人员认为这不是Bug而是预期行为，或者无法复现，你会怎么做？
首先，不简单地认为开发人员错了，而是会采取合作的态度：

提供更清晰的证据：
重新检查我的测试环境、步骤和数据，确保复现步骤是100%准确且可重复的。
提供更详细的日志、截图、甚至是屏幕录像。

深入理解需求：
与开发人员和产品经理一起回顾需求文档，确认我们对需求的理解是否一致。
可能是我理解有误，也可能是需求文档本身描述不清晰。

现场复现：
邀请开发人员到我的测试环境，或者我将测试环境配置信息完整地提供给他，亲自演示Bug的复现过程。

分析根因：
如果可能，我会尝试进行初步的根因分析，比如通过 strace, gdb 等工具，将问题定位到更具体的函数或代码行，这能极大地帮助开发人员快速解决问题。
目标是共同保证产品质量，而不是‘抓Bug’。
通过提供无可辩驳的证据和建设性的沟通，绝大多数分歧都能得到有效解决。
---------------------------------------------------------------------------
三、 自动化与工具链 (考察实战经验和工具使用能力)
5. 问题：你有使用Shell或Python进行自动化测试的经验。
请举例说明你如何用Python（或Shell）编写一个自动化测试脚本，来测试一个字符设备驱动（比如/dev/my_char_dev）的读写功能。
假设需要测试一个简单的字符设备，会用Python的 os 模块进行系统调用。脚本的大致框架如下：
#!/usr/bin/env python3
import os

# 定义设备节点
dev_path = '/dev/my_char_dev'

# 测试数据
test_data = b"Hello, Driver! This is a test string."

try:
    # 1. 打开设备
    fd = os.open(dev_path, os.O_RDWR)
    print(f"Device {dev_path} opened successfully.")

    # 2. 写入数据
    bytes_written = os.write(fd, test_data)
    print(f"Written {bytes_written} bytes: {test_data}")

    # 3. 定位到文件开头（为读取做准备）
    os.lseek(fd, 0, os.SEEK_SET)

    # 4. 读取数据
    read_data = os.read(fd, len(test_data)) # 读取同样长度的数据
    print(f"Read {len(read_data)} bytes: {read_data}")

    # 5. 验证数据是否正确
    if read_data == test_data:
        print("PASS: Data read back correctly.")
    else:
        print("FAIL: Data mismatch!")

    # 6. 关闭设备
    os.close(fd)
    print("Device closed.")

except OSError as e:
    print(f"Operation failed: {e}")
实际项目中，会将其封装成函数，加入日志记录、异常处理、参数化（测试不同数据）、以及与CI工具（如Jenkins）集成。
---------------------------------------------------------------------------
6. 问题：职位要求中提到Gerrit, Jenkins, Docker, Jira。请谈谈你如何在日常工作中使用这些工具，并描述它们是如何协作的。
Jira：
用于需求、任务和缺陷跟踪。
会在Jira上创建测试任务，接收开发提测的版本号，并将发现的Bug提交到Jira，指派给相应开发人员，跟踪其状态从‘新建’到‘已解决’，最后进行回归测试并关闭。

Gerrit：
代码审查工具。开发人员提交的驱动代码会推送到Gerrit，和其他同事会进行代码评审，不仅看功能，也会关注代码中可能存在的可测试性问题和潜在风险。

Jenkins：
持续集成/交付的核心。我会配置Jenkins 自动化任务：
1）监听Gerrit的代码合并或定时触发。
2）自动从仓库拉取最新代码，编译生成内核和驱动模块。
3）启动Docker容器或在预置的物理测试机上部署新版本驱动。
4）自动执行我编写的自动化测试脚本集。
5）收集测试结果和日志，生成报告并邮件通知团队。

Docker：
主要用它来创建标准化、可复现的测试环境。
通过Dockerfile定义包含特定内核版本、依赖库和测试工具的环境镜像，可以保证每个测试运行的环境完全一致，
避免了‘在我机器上是好的’这类问题。Jenkins会拉起这些容器来执行测试。

它们的协作流程是：
代码 -> Gerrit (Review) -> 合并 -> Jenkins (触发构建和测试) 
-> 测试报告 -> (如果失败) Jira (创建Bug)，形成一个高质量的自动化闭环。
---------------------------------------------------------------------------
四、 情景与编程题 (考察综合能力和动手能力)
7. 问题：【编程题】请用Shell脚本编写一个简单的压力测试工具，循环对某个驱动设备节点进行1000次‘打开-写入-读取-验证-关闭’的操作，并在任何一次失败时记录日志并退出。
#!/bin/bash

DEVICE="/dev/my_device"
TEST_STR="Pressure Test Data"
LOG_FILE="pressure_test.log"
COUNT=1000

# 检查设备是否存在
if [[ ! -c $DEVICE ]]; then
    echo "Error: Device $DEVICE does not exist!" | tee -a $LOG_FILE
    exit 1
fi

echo "Starting pressure test for $COUNT iterations..." | tee -a $LOG_FILE

for ((i=1; i<=$COUNT; i++))
do
    # 打开设备并获取文件描述符
    if ! exec 3<> $DEVICE; then
        echo "Iteration $i FAILED: Cannot open device." | tee -a $LOG_FILE
        exit 2
    fi

    # 写入数据
    if ! echo -n "$TEST_STR" >&3; then
        echo "Iteration $i FAILED: Write error." | tee -a $LOG_FILE
        exec 3>&- # 关闭FD
        exit 3
    fi

    # 读取数据
    if ! read -r -u 3 -n ${#TEST_STR} result; then
        echo "Iteration $i FAILED: Read error." | tee -a $LOG_FILE
        exec 3>&-
        exit 4
    fi

    # 验证数据
    if [[ "$result" != "$TEST_STR" ]]; then
        echo "Iteration $i FAILED: Data mismatch. Expected: '$TEST_STR', Got: '$result'" | tee -a $LOG_FILE
        exec 3>&-
        exit 5
    fi

    # 关闭设备
    exec 3>&-

    # 打印进度
    if (( i % 100 == 0 )); then
        echo "Iteration $i completed successfully."
    fi
done

echo "Pressure test PASSED all $COUNT iterations!" | tee -a $LOG_FILE
exit 0
---------------------------------------------------------------------------
8. 问题：在性能测试中，你发现驱动的一个IO操作延迟很高。你可能会使用哪些Linux工具来定位性能瓶颈？请简述你的分析思路。
系统层面：
首先使用 top 或 htop 查看整体CPU、IOwait情况。
使用 iostat -x 1 查看磁盘的利用率（%util）、响应时间（await）、队列长度（aqu-sz），判断瓶颈是否在硬件IO。

进程/函数层面：
如果CPU高，我会使用 perf 工具对系统或特定进程进行采样分析。
perf top 实时查看热点函数。
perf record -g -p <pid> 记录调用栈，
然后用 perf report 分析，看时间主要消耗在驱动代码、内核通用代码还是用户空间，这能直接定位到耗时的函数。

内核轨迹跟踪：
对于更底层的延迟分析，我会使用 ftrace 或 bpftrace 来跟踪内核函数的调用延迟。
例如，设置跟踪 irq_handler, softirq, 驱动中的 read/write 函数，测量它们从开始到结束的时间，
从而判断延迟具体发生在哪个阶段（硬件中断处理、软中断、锁竞争、内存分配等）。

锁竞争分析：
如果怀疑是锁的问题，可以使用 lockstat 或 perf lock 来分析自旋锁或互斥锁的竞争情况。
通过组合使用这些工具，我可以一步步将高性能延迟的根因定位到具体的代码行或系统组件。
---------------------------------------------------------------------------
代码 -> Gerrit (Review) -> 合并 -> Jenkins (触发构建和测试) -> 测试报告 -> (如果失败) Jira (创建Bug)，形成一个高质量的自动化闭环。

Jenkins执行CI任务
拉取代码&编译  ->  构建Docker测试环境  -> 执行自动化测试  ->  生成测试报告
||                                                     ^
VV                                                     |
测试是否通过？                                          |
通过-> 生成版本发布物料 -> end                           |
不通过 -> Jenkins调用Jira API创建Bug                    |
             |                                         |
             v                                         |
          Jira生成Bug工单指派给对应的开发者              |
             |                                         |
             v                                         | 
          开发者提交代码至Gerrit                         |
             |  ^                                      |
             |  | 评审拒绝                              |
             v  |                                      | 
          Gerrit触发代码评审    -----评审通过&合并------>

Demo：
第一步：认为来源与规划（Jira）
1、关键设定：Jira中，会有一个对应功能需求的故事（story）与任务（Task），如NET-125。
2、操作（测试）：
   在Jira任务下创建子任务NET-125-TEST，将其分配给自己。
   阅读需求，编写和准备相应的自动化测试脚本，验证相关测试功能。这些自动化脚本会放入代码库或者被Jenkins调用。
   开发者完成代码后，会在Jira上将状态更新为“解决”或“待测”，并在评论中提供Gerrit代码的评审链接和编译后的驱动版本。
第二步：代码审查 (Gerrit)
1、关键设定： Gerrit项目配置了必须至少+2票才能合并的规则，
   并且会自动触发一个名为 net-driver-verify 的Jenkins任务进行初步编译。
2、操作（评审者）：
   收到Gerrit的评审邮件，点击链接查看这次代码提交。
   不仅评审功能逻辑，更关注可测试性：
            代码中是否新增了可供查询的调试接口？（例如，在 /sys/class/net/eth0/features/tso 暴露状态）
            是否有清晰的日志输出？（方便失败时定位问题）
            是否考虑了异常情况？（这些正是我测试用例的来源）
   如果发现可测试性问题，会在Gerrit中提交评论，要求开发者补充，并投-1 票。如果一切良好，投 +1 或 +2 票。
第三步：持续集成与自动化测试 (Jenkins + Docker)
1、关键设定（Jenkins Job配置）：
    源码管理： Git，仓库地址指向Gerrit。
    构建触发器：
              Gerrit Trigger: 配置为当refs/heads/master分支有新的合并时触发。
              同时配置定时构建，例如每晚一次，进行全量回归测试。
    构建环境： 选择提供Docker环境或使用Docker Pipeline插件。
    构建步骤：
              Shell Step 1: 编译
                    make ARCH=x86_64 -j4
                    make modules_install INSTALL_MOD_PATH=./output
                    # 将编译好的驱动（.ko文件）和内核镜像打包成tar包
              Shell Step 2: 准备测试环境并执行测试
                    # 使用Dockerfile构建测试镜像
                    docker build -t net-driver-test-env:latest -f Dockerfile.test .

                    # 运行容器，将编译输出的驱动包、测试脚本挂载到容器内
                      docker run --rm --privileged \
                      -v $(pwd)/output:/output \
                      -v $(pwd)/scripts:/scripts \
                      net-driver-test-env:latest \
                      /scripts/run_net_tests.sh # 在容器内执行自动化测试脚本
    后构建操作：
               Publish JUnit test result: 配置XML报告路径（例如 test-results/*.xml），Jenkins会解析并展示图表。
               Editable Email Notification: 将测试报告和日志打包作为附件，发送给开发团队。
2、关键设定（Dockerfile.test）：
     # 使用一个基础Linux镜像
     FROM ubuntu:20.04

     # 安装测试依赖：内核头文件、编译工具、测试工具（ethtool, iperf3, tc等）
     RUN apt-get update && apt-get install -y \
         linux-headers-generic \
         build-essential \
         ethtool \
         iperf3 \
         iproute2 \
         && rm -rf /var/lib/apt/lists/*

     # 设置工作目录
       WORKDIR /test
       CMD ["/bin/bash"]
3、我的自动化测试脚本 (run_net_tests.sh)：
     #!/bin/bash
     # 加载新编译的驱动
     insmod /output/lib/modules/.../xx_net_driver.ko
     
     # 等待网卡识别
     sleep 2
     
     # 使用ethtool验证TSO是否启用
     if ethtool -k eth0 | grep -q "tcp-segmentation-offload: on"; then
         echo "TSO is enabled: PASS" > test_result.xml
     else
         echo "TSO is NOT enabled: FAIL" > test_result.xml
         exit 1 # 非0退出码表示失败，会触发Jenkins构建失败
     fi

     # ... 可以继续执行其他性能或功能测试

第四步：问题跟踪与闭环 (Jira + Jenkins)
1、测试通过：
     Jenkins任务显示蓝色（成功）。
     团队收到邮件：“驱动NET构建 #123 成功”，所有测试通过。这意味着功能已就绪，可以发布。
2、测试失败：
     Jenkins任务显示红色（失败）。
     关键设定： 在Jenkins中配置 Jira Plugin，并填写Jira站点的URL、用户名、密码用于API调用。
     自动化闭环： 在Jenkins的后构建操作中配置：
           Jira: Create Issue
           Project: NET
           Issue Type: Bug
           Description: 自动填充失败日志摘要和构建链接。
           Assignee: 根据代码提交者自动指派给对应的开发者。
     开发者会立即收到Jira提醒，有一个新的Bug NET-128: TSO功能测试失败于构建#123 被指派给他。
     他点击Jira中的链接直接跳转到Jenkins失败日志页面，开始排查问题。
     开发者修复后，再次提交代码到Gerrit，新的循环开始，直到所有问题解决。
---------------------------------------------------------------------------
